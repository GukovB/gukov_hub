{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6e05b6-1498-480f-81b8-25b9410030f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import GC\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from random import sample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.cli import LightningCLI\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torchmetrics\n",
    "\n",
    "from dinuc import *\n",
    "\n",
    "from ambrosini_auc import ambrosini_roc_auc_score\n",
    "import sarus_wrapper\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import tempfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f638790b-b6fe-4429-a2c0-631b671766c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "chr_val = 'chr7'\n",
    "chr_test = 'chr11'\n",
    "seed_everything(42, workers=True)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "intervals = sorted(glob.glob(\"/home/jakewayd/New_ChIP-Seq/data/CHS.Schmitges/Train_intervals/*.peaks\"))\n",
    "window = 300\n",
    "genome = SeqIO.to_dict(SeqIO.parse('data/CHS.Schmitges/chrs.fasta', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee766ab2-7bff-40e6-ac56-f8a1e5133cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta(x, shift = 0, shuffle = None, dinuc = None):\n",
    "    \n",
    "    chrom = x['#CHROM']\n",
    "    new_peak = x['abs_summit']+np.random.choice([shift, -shift])\n",
    "    seq = genome[chrom][new_peak-window//2:new_peak+window//2+1].seq.__str__().upper()\n",
    "    \n",
    "    if shift != 0 or shuffle or dinuc: \n",
    "        y = 0\n",
    "    else: \n",
    "        y = 1\n",
    "    \n",
    "    if set(seq) - set('ATGC'):\n",
    "        return None, None, None\n",
    "    \n",
    "    if shuffle:\n",
    "        seq = ''.join(random.sample(seq, len(seq)))\n",
    "        \n",
    "    if dinuc:\n",
    "        seq = shuffle_string_dinucl(seq)\n",
    "    \n",
    "    return chrom, seq, y\n",
    "\n",
    "def get_df(df, method, shift = 0):\n",
    "    \n",
    "    if method == 'pos':\n",
    "        seqs = df.apply(lambda x: get_fasta(x), axis = 1)\n",
    "    \n",
    "    if method == 'shift':\n",
    "        seqs = df.apply(lambda x: get_fasta(x, shift = shift), axis = 1)\n",
    "        \n",
    "    if method == 'shuffle':\n",
    "        seqs = df.apply(lambda x: get_fasta(x, shuffle = True), axis = 1)\n",
    "        \n",
    "    if method == 'dinuc':\n",
    "        seqs = df.apply(lambda x: get_fasta(x, dinuc = True), axis = 1)\n",
    "        \n",
    "    seqs = pd.DataFrame(seqs.to_list(), columns=['chr', 'seq', 'y']).dropna().reset_index(drop = True)\n",
    "    return seqs\n",
    "\n",
    "def get_datasets(peaks, method, shift = 0):\n",
    "    \n",
    "    df = pd.read_csv(peaks, sep = '\\t')\n",
    "    df = df[['#CHROM', 'abs_summit']]\n",
    "    for i in [window//4, -window//4]:\n",
    "        data = df.copy()\n",
    "        data['abs_summit'] = data['abs_summit']+i\n",
    "        df = pd.concat([df, data]).reset_index(drop = True)\n",
    "        \n",
    "    df_pos = get_df(df, 'pos')\n",
    "    df_neg = get_df(df, method, shift)\n",
    "    \n",
    "    return pd.concat([df_pos, df_neg]).reset_index(drop = True)\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    \n",
    "    nuc_d = {\n",
    "         'A':[1.0,0.0,0.0,0.0],\n",
    "         'C':[0.0,1.0,0.0,0.0],\n",
    "         'G':[0.0,0.0,1.0,0.0],\n",
    "         'T':[0.0,0.0,0.0,1.0]\n",
    "    }\n",
    "        \n",
    "    return np.array([nuc_d[x] for x in seq])\n",
    "\n",
    "class SeqDatasetOHE(Dataset):\n",
    "    def __init__(self, df, seq_col='seq', target_col='y'):\n",
    "        \n",
    "        self.seqs = list(df[seq_col].values)\n",
    "        self.seq_len = len(self.seqs[0])\n",
    "        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n",
    "        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        seq = self.ohe_seqs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return seq, label\n",
    "    \n",
    "class DataModuleSeqs(pl.LightningDataModule):\n",
    "    def __init__(self, peaks, method, shift = 0, batch_size = 512):\n",
    "        super().__init__()\n",
    "        self.peaks = peaks\n",
    "        self.method = method\n",
    "        self.shift = shift\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        df = get_datasets(self.peaks, self.method, self.shift)\n",
    "        df = df.sample(frac = 1)\n",
    "        \n",
    "        df_val = df[df['chr'] == chr_val]\n",
    "        if df_val.shape[0] < 1000:\n",
    "            return None\n",
    "        \n",
    "        df_train = df[(df['chr'] != chr_val) & (df['chr'] != chr_test)]\n",
    "        df_test = df[df['chr'] == chr_test]\n",
    "        \n",
    "        print(df_val['y'].value_counts())\n",
    "        \n",
    "        self.train = SeqDatasetOHE(df_train)\n",
    "        self.val = SeqDatasetOHE(df_val)\n",
    "        self.test = SeqDatasetOHE(df_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle = True, num_workers = 5)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, shuffle = False, num_workers = 5)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle = False, num_workers = 5)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle = False, num_workers = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912cbdf-b251-40d5-a455-352b7f7d29c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### классы для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b87712-0544-4a2a-8af7-8d5b2355a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, filters):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(filters, filters, kernel_size = 3, padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.norm1 = nn.BatchNorm1d(filters)\n",
    "        self.activ = nn.SELU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(filters, filters, kernel_size = 3, padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.norm2 = nn.BatchNorm1d(filters)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activ(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activ(x + out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e093bb0a-5114-48e4-ad50-6064d362fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Truck(nn.Module):\n",
    "    def __init__(self, filters, n_blocks):\n",
    "        super(Truck, self).__init__()\n",
    "        truck = []\n",
    "        for i in range(n_blocks):\n",
    "            truck += [Block(filters)]\n",
    "            \n",
    "        self.truck = nn.Sequential(*truck)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.truck(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d070b6cd-dd91-482a-b02a-9946104df032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers(nn.Module):\n",
    "    def __init__(self, filters, n_blocks, n_layers):\n",
    "        super(Layers, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Sequential(\n",
    "                    Truck(pow(2, i)*filters, n_blocks),\n",
    "                    nn.Conv1d(pow(2, i)*filters, pow(2, i+1)*filters, kernel_size = 3, padding = 1, stride = 2),\n",
    "                    nn.SELU(inplace=True)\n",
    "                )\n",
    "            ]       \n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265ec00-9abc-4c70-81c6-be223f1f789a",
   "metadata": {},
   "source": [
    "### модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea22eb9-999d-4f00-97ea-8b754cb86e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_like(nn.Module):\n",
    "    def __init__(self, filters = 8, num_in_block = 2, num_in_truck = 2, learning_rate = 1e-3):\n",
    "        super(Resnet_like, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(4, filters, kernel_size = 7)\n",
    "        self.bn1  = nn.BatchNorm1d(filters)\n",
    "        self.SELU = nn.SELU(inplace=True)\n",
    "        \n",
    "        self.layers = Layers(filters, num_in_block, num_in_truck)\n",
    "        \n",
    "        self.avg = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(pow(2, num_in_truck)*filters, pow(2, num_in_truck+1)*filters),\n",
    "            nn.BatchNorm1d(pow(2, num_in_truck+1)*filters),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Linear(pow(2, num_in_truck+1)*filters, pow(2, num_in_truck+2)*filters),\n",
    "            nn.BatchNorm1d(pow(2, num_in_truck+2)*filters),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Linear(pow(2, num_in_truck+2)*filters, 1)\n",
    "        )\n",
    "        \n",
    "        self.name = f'{type(self).__name__}_{filters}_{num_in_truck}_{num_in_block}'\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.SELU(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e732b213-d162-402c-b956-3d75b84d3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self, model, len_train, lr = 1e-3, weight_decay = 1e-4, num_classes = 2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.len_train = len_train\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = model\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.train_auc = AUROC(task = 'binary')\n",
    "        self.val_auc = AUROC(task = 'binary')\n",
    "        self.name = self.model.name\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.float())\n",
    "        loss = self.loss_fn(y_hat, y.float())\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_auc', self.train_auc(y_hat, y.float()), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.float())\n",
    "        loss = self.loss_fn(y_hat, y.float())\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_auc', self.val_auc(y_hat, y.float()), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.float())\n",
    "        loss = self.loss_fn(y_hat, y.float())\n",
    "        \n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_auc', self.val_auc(y_hat, y.float()))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, epochs=self.trainer.max_epochs, steps_per_epoch=self.len_train),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        \n",
    "        # reduce_on_plateau_scheduler = {\n",
    "        #     'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3),\n",
    "        #     'monitor': 'val_loss',\n",
    "        #     'interval': 'epoch',\n",
    "        #     'frequency': 1,\n",
    "        #     'strict': True\n",
    "        # }\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064de2f6-148e-41c1-9a80-774940d23cf0",
   "metadata": {},
   "source": [
    "### Приколы с pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45b9f69-6760-4833-8404-467bc8a67b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weigths(x):\n",
    "    if 'Conv1d' in str(type(x)) or 'Linear' in str(type(x)):\n",
    "        torch.nn.init.xavier_uniform_(x.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1100f970-8dc9-4bc9-babd-20d207c8430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(model, neg, dataset):\n",
    "    \n",
    "    df = pd.read_csv(f'logs/{model}/{neg}/{dataset}/metrics.csv')\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        subplot_titles = ('Training and validation losses', 'Training and validation AUCs',\n",
    "                          'Learning rate every batch', 'Result AUCs')\n",
    "    )\n",
    "\n",
    "    colors = px.colors.qualitative.Pastel\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df['epoch'].dropna().unique(),\n",
    "            y = df['train_loss_epoch'].dropna(),\n",
    "            marker_color = colors[0],\n",
    "            name = 'train_loss_epoch'\n",
    "        ), row = 1, col = 1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df['epoch'].dropna().unique(),\n",
    "            y = df['val_loss_epoch'].dropna(),\n",
    "            marker_color = colors[1],\n",
    "            name = 'val_loss_epoch'\n",
    "        ), row = 1, col = 1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df['epoch'].dropna().unique(),\n",
    "            y = df['train_auc_epoch'].dropna(),\n",
    "            marker_color = colors[2],\n",
    "            name = 'train_auc_epoch'\n",
    "        ), row = 1, col = 2\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df['epoch'].dropna().unique(),\n",
    "            y = df['val_auc_epoch'].dropna(),\n",
    "            marker_color = colors[3],\n",
    "            name = 'val_auc_epoch'\n",
    "        ), row = 1, col = 2\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = [i for i in range(len(df['lr-AdamW'].dropna()))],\n",
    "            y = df['lr-AdamW'].dropna(),\n",
    "            marker_color = colors[4],\n",
    "            name = 'lr-AdamW'\n",
    "        ), row = 2, col = 1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = ['train', 'val', 'test'],\n",
    "            y = [df['train_auc_epoch'].dropna().to_list()[-1],\n",
    "                 df['val_auc_epoch'].dropna().to_list()[-1],\n",
    "                 df['test_auc'].dropna().to_list()[-1]],\n",
    "            text = [round(df['train_auc_epoch'].dropna().to_list()[-1], 3),\n",
    "                 round(df['val_auc_epoch'].dropna().to_list()[-1], 3),\n",
    "                 round(df['test_auc'].dropna().to_list()[-1], 3)],\n",
    "            marker_color = colors[5],\n",
    "            name = 'result'\n",
    "        ), row = 2, col = 2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin = dict(t = 120, b = 5, l = 5, r = 5),\n",
    "        template = 'plotly_white',\n",
    "        title = {\n",
    "            'text': f\"<b>Model:</b> {model} <br> <b>Negative:</b> {neg} <br> <b>Dataset</b>: {dataset} <br>\",\n",
    "            'x': 0.01,\n",
    "            'y': 0.95,\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        height = 700,\n",
    "        width = 1200,\n",
    "        xaxis1 = dict(title = 'Epoch'),\n",
    "        yaxis1 = dict(title = 'BCEloss logit'),\n",
    "        xaxis2 = dict(title = 'Epoch'),\n",
    "        yaxis2 = dict(title = 'AUC'),\n",
    "        xaxis3 = dict(title = 'Step'),\n",
    "        yaxis3 = dict(title = 'Learning rate'),\n",
    "        xaxis4 = dict(title = 'Dataset'),\n",
    "        yaxis4 = dict(title = 'AUC'),\n",
    "    )\n",
    "\n",
    "    fig.write_image(f'logs/{model}/{neg}/{dataset}/plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50615fb6-80ca-4f5d-95d8-a7cdebfb1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_model(num_filters, blocks, divs, intervals, method, shift):\n",
    "    d = {'name': [], 'neg': [], 'model': [], 'auc': []}\n",
    "    for name in intervals:\n",
    "        print(name.split('/')[-1].split('.')[0])\n",
    "        datasets = DataModuleSeqs(name, method, shift)\n",
    "        \n",
    "        try:\n",
    "            datasets.val\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        model = Resnet_like(num_filters, blocks, divs).apply(init_weigths)\n",
    "        shell = MyModel(model, len(datasets.train_dataloader()))\n",
    "        if not os.path.exists(f'logs/{shell.name}/{method}_{shift}/{name.split(\"/\")[-1]}'):\n",
    "            os.makedirs(f'logs/{shell.name}/{method}_{shift}/{name.split(\"/\")[-1]}')\n",
    "            \n",
    "        trainer = pl.Trainer(\n",
    "                             max_epochs = 20,\n",
    "                             accelerator='gpu',\n",
    "                             devices=[1],\n",
    "                             auto_lr_find = True,\n",
    "                             callbacks = [EarlyStopping(monitor = \"val_loss\", mode = \"min\"), \n",
    "                                          LearningRateMonitor(logging_interval='step')],\n",
    "                             log_every_n_steps = 1,\n",
    "                             logger = CSVLogger(save_dir = f\"logs/{shell.name}\", name = f'{method}_{shift}', version = name.split('/')[-1])\n",
    "                            )\n",
    "\n",
    "        trainer.fit(model = shell, datamodule = datasets)\n",
    "        result = trainer.validate(model = shell, datamodule = datasets)\n",
    "        test = trainer.test(model = shell, datamodule = datasets)\n",
    "        \n",
    "        plot_all(shell.name, f'{method}_{shift}', name.split('/')[-1])\n",
    "        d['auc'].append(test[0]['test_auc'])\n",
    "        d['name'].append(name)\n",
    "        d['neg'].append(method)\n",
    "        d['model'].append(shell.name)\n",
    "    \n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36034070-8d75-42f7-8bea-bf7ef7a7ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_valid_model(16, 1, 1, intervals[5:6], 'dinuc', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b792f5-e49d-42b4-8a6e-25494810e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWMModel:\n",
    "    def __init__(self, matrix_path, transpose=False):\n",
    "        self.matrix_path = matrix_path\n",
    "        self.transpose_mode = transpose\n",
    "        self.dir = tempfile.TemporaryDirectory()\n",
    "    \n",
    "    def fit(self, sequences: List[str], y: List[int]):\n",
    "        raise NotImplementedError(\"fitting PWMModel is not implemented\")\n",
    "    \n",
    "    def predict(self, sequences: List[str]):\n",
    "        records = [SeqRecord(Seq(seq), id=f\"{i}\", description=\"\") for i, seq in enumerate(sequences, 1)]\n",
    "        file_path = os.path.join(self.dir.name, \"predict.fasta\")\n",
    "        with open(file_path, \"wt\") as outfile:\n",
    "            SeqIO.write(records, outfile, \"fasta\")\n",
    "        pred = self.predict_file(file_path)\n",
    "        return pred\n",
    "    \n",
    "    def predict_file(self, file_path: str):\n",
    "        sarus_result = sarus_wrapper.launch_sarus_single(file_path, self.matrix_path, transpose=self.transpose_mode)\n",
    "        scores = sarus_result[\"val\"].values\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec73b36-1dea-45b4-a3fb-b01c83098010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_string(s):\n",
    "    chars = list(s)\n",
    "    np.random.shuffle(chars)\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93aafc1-f49e-427c-bbfe-a488368b81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_records(type_dir, type_, subtype=\"-\"):\n",
    "    motifs_dir = \"/home/arsen_l/imtf/wp04_datachecker/bestpwms/greco-motifs/best-mx\"\n",
    "    all_type = \"ALL\"\n",
    "    data_type = \"CHS\"\n",
    "    has_subdirs = False\n",
    "    records = list()\n",
    "    motif_mask = os.path.join(type_dir, \"*.p?m\")\n",
    "    motif_paths = glob.glob(motif_mask)\n",
    "    motif_paths.sort()\n",
    "    for motif_path in motif_paths:\n",
    "        record = dict()\n",
    "        record[\"type\"] = type_\n",
    "        record[\"subtype\"] = subtype\n",
    "        motif_id = os.path.basename(motif_path)\n",
    "        record[\"factor\"] = motif_id.split(\"@\", maxsplit=1)[0].split(\".\", maxsplit=1)[0]\n",
    "        record[\"motif_id\"] = motif_id\n",
    "        record[\"extension\"] = os.path.splitext(record[\"motif_id\"])[-1]\n",
    "        record[\"path\"] = motif_path\n",
    "        records.append(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0977ca21-15c4-4a01-a249-cd97bfc57f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_df():\n",
    "    motifs_dir = \"/home/arsen_l/imtf/wp04_datachecker/bestpwms/greco-motifs/best-mx\"\n",
    "    all_type = \"ALL\"\n",
    "    data_type = \"CHS\"\n",
    "    has_subdirs = False\n",
    "    motif_records = list()\n",
    "    if has_subdirs:\n",
    "        subtype_mask = os.path.join(motifs_dir, data_type, \"*\")\n",
    "        subtype_paths = sorted(glob.glob(subtype_mask))\n",
    "        for subtype_path in subtype_paths:\n",
    "            subtype = os.path.basename(subtype_path)\n",
    "            records = get_motif_records(subtype_path, data_type, subtype)\n",
    "            motif_records.extend(records)\n",
    "    else:\n",
    "        data_type_dir = os.path.join(motifs_dir, data_type)\n",
    "        records = get_motif_records(data_type_dir, data_type)\n",
    "        motif_records.extend(records)\n",
    "\n",
    "    records = get_motif_records(data_type_dir, all_type)\n",
    "    motif_records.extend(records)\n",
    "\n",
    "    motif_df = pd.DataFrame(motif_records).sort_values(by=[\"factor\"]).reset_index(drop=True)\n",
    "    return motif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6babc8b-bf8e-4398-b703-11aba08aab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quals_pwm(names, neg, shift = 0, motif_df = None):\n",
    "    motifs_dir = \"/home/arsen_l/imtf/wp04_datachecker/bestpwms/greco-motifs/best-mx\"\n",
    "    all_type = \"ALL\"\n",
    "    data_type = \"CHS\"\n",
    "    has_subdirs = False\n",
    "    metrics = {\n",
    "                \"AUROC\": (roc_auc_score, dict()),\n",
    "              }\n",
    "\n",
    "    results = list()\n",
    "    for name in tqdm(names, total=len(names), ascii=True):\n",
    "\n",
    "        df = get_datasets(name, neg, shift)\n",
    "        df = df.sample(frac = 1)    \n",
    "        if df[df['chr'] == chr_val].shape[0] < 1000:\n",
    "            continue\n",
    "\n",
    "        dataset_id_pos = os.path.basename(name)\n",
    "        print(dataset_id_pos)\n",
    "        factor = dataset_id_pos.split('.')[0]\n",
    "        if factor not in motif_df[\"factor\"].values:\n",
    "            results.append({\"factor\": factor, \"dataset_positive\": dataset_id_pos, \"dataset_negative_type\": neg})\n",
    "            print(f\"skipped {factor}\")\n",
    "            continue\n",
    "\n",
    "        positive = df[df['y'] == 1]['seq'].to_list()\n",
    "        negative = df[df['y'] == 0]['seq'].to_list()\n",
    "        eval_set = positive + negative\n",
    "        y_true = np.concatenate([np.ones(shape=len(positive)), np.zeros(shape=len(negative))])\n",
    "\n",
    "        subdf = motif_df[motif_df[\"factor\"] == factor]\n",
    "        for _, record in subdf.iterrows():\n",
    "            pwm_path = record[\"path\"]\n",
    "            pwm_id = record[\"motif_id\"]\n",
    "\n",
    "            pwm_model = PWMModel(pwm_path)\n",
    "            scores = pwm_model.predict(eval_set)\n",
    "\n",
    "            eval_result = record.to_dict()\n",
    "            eval_result[\"dataset_positive\"] = dataset_id_pos\n",
    "            eval_result[\"dataset_negative\"] = neg\n",
    "            for metric_name, (metric, params) in metrics.items():\n",
    "                m_v = metric(y_true=y_true, y_score=scores, **params)\n",
    "            eval_result[metric_name] = m_v\n",
    "        results.append(eval_result)\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dacb36b-0386-4605-8e12-c49cd3a75ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quals(intervals, method, shift):\n",
    "    motifs_dir = \"/home/arsen_l/imtf/wp04_datachecker/bestpwms/greco-motifs/best-mx\"\n",
    "    all_type = \"ALL\"\n",
    "    data_type = \"CHS\"\n",
    "    has_subdirs = False\n",
    "    motif_df = get_motif_df()\n",
    "    return get_quals_pwm(intervals, method, motif_df = motif_df, shift = shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d438f5f1-1939-4312-b866-cfb9f64489d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_in(num_filters, blocks, divs, shifts, intervals, overwrite = False):\n",
    "    for i in num_filters:\n",
    "        for j in blocks:\n",
    "            for k in divs:\n",
    "                for method in ['shuffle', 'dinuc', 'shift']:\n",
    "                    for shift in shifts:\n",
    "                        if not os.path.exists(f'./results/{method}_{shift}_{i}_{j}_{k}.tsv') or overwrite:\n",
    "                            result = train_valid_model(i, j, k, intervals, method, shift)\n",
    "                            result.to_csv(f'./results/{method}_{shift}_{i}_{j}_{k}.tsv', sep = '\\t')\n",
    "\n",
    "                        if not os.path.exists(f'./quals/{method}_{shift}.tsv') or overwrite:\n",
    "                            quals = get_quals(intervals, method, shift)\n",
    "                            quals.to_csv(f'./quals/{method}_{shift}.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e53f7-55d6-4085-ba7e-864f4078611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLI4\n",
      "SNAI1\n",
      "SNAI1\n",
      "SNAI1\n",
      "ZFP28\n",
      "ZFP28\n",
      "0.0    512\n",
      "1.0    512\n",
      "Name: y, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | Resnet_like       | 14.6 K\n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "2 | acc       | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "14.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.6 K    Total params\n",
      "0.059     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01698136329650879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('step', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014183759689331055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240c4b33dd424899b31f6999d556a2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015544414520263672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018306493759155273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015924930572509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01569652557373047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013447046279907227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3681a6d6e94c3e96425bab4295d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "          step                      5.0\n",
      "      val_acc_epoch            0.9208984375\n",
      "        val_auroc             0.989990234375\n",
      "     val_loss_epoch         0.17394708096981049\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ZFP3\n",
      "ZIM3\n",
      "1.0    4888\n",
      "0.0    4888\n",
      "Name: y, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | Resnet_like       | 14.6 K\n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "2 | acc       | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "14.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.6 K    Total params\n",
      "0.059     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013719320297241211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023631811141967773,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df396e6bc748928af0a91b1e57a893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01430654525756836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014724969863891602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f9a8b1970d4f24a08f8cfaa200a8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakewayd/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01423025131225586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698021717e65444daf6fa18b19bf8c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_in([8, 16, 24], [1, 2, 3], [1, 2, 3], [0, 100, 500], intervals, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd8bf3-5e0a-4416-87cc-ff3d47f30b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
